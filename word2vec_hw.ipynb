{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Word Embedding Network\n",
    "In this assignment you will practice how to create a Word Embedding Network in Tensorflow 2.0. First, you will finish some functions to parse the data, build the corpus and construct the skip pair. Then, you will construct a word embedding network by follow the specific requirements and architectures. Finally, you will train the network and visualize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, BatchNormalization\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split_sentence\n",
    "1. Remove the special characters from the sentence\n",
    "2. Filter the short sentence\n",
    "\n",
    "You can rewrite this function or add more filter conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentence(sentences):\n",
    "    new_sentence = list()\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.replace(\"\\n\", \" \")\n",
    "        sentence = sentence.replace(\",\", \"\")\n",
    "        sentence = sentence.replace(\"\\'\", \" \")\n",
    "        sentence = sentence.replace(\"?\", \"\")\n",
    "        sentence = sentence.replace(\"!\", \"\")\n",
    "        sentence = sentence.replace(\";\", \"\")\n",
    "        sentence = sentence.lower()\n",
    "        if sentence.count(\" \") <= 3:\n",
    "            continue\n",
    "        new_sentence.append(sentence)\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all the sentences from the input file. Split the input into each sentence by calling the \"split_sentence\" function.\n",
    "\n",
    "- test_doc_short: Small dataset. You can use it to debug your code.\n",
    "\n",
    "- test_doc_long: Large dataet. You should use it to get the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "['he is the king ', '  he is the royal king ', '  the king is royal ']\n"
     ]
    }
   ],
   "source": [
    "file = open(\"test_doc_short.txt\",'r')\n",
    "raw_data_1 = file.read()\n",
    "file.close()\n",
    "sentences = raw_data_1.split(\".\")\n",
    "print (len(sentences))\n",
    "corpus_raw = list()                  \n",
    "corpus_raw = split_sentence(sentences)\n",
    "print (corpus_raw[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build_dictionary (10 points)\n",
    "1. Extract the word from the input. \n",
    "2. Build a non-duplicate word dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(corpus_raw):\n",
    "    words = []\n",
    "    # TO DO\n",
    "    \n",
    "    # larger set that contains all the non-repeated words so far\n",
    "    tempset = set([])\n",
    "    \n",
    "    for line in corpus_raw:\n",
    "        temp = line.split(\" \")\n",
    "        for word in temp:\n",
    "            # single set that only contains the current word\n",
    "            singleset = set([])\n",
    "            if word != \"\":\n",
    "                singleset.add(word)\n",
    "                # if single set is a subset of the larger set, means repeated, ignore\n",
    "                if singleset.issubset(tempset):\n",
    "                    continue\n",
    "                else:\n",
    "                    tempset.add(word)\n",
    "                    words.append(word)\n",
    "\n",
    "    # END TO DO\n",
    "    return set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dict = build_dictionary(corpus_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The number of word in \"test_doc_long\" dataset is around 7.\n",
    "- The number of word in \"test_doc_long\" dataset is around 1831."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print (len(corpus_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one_hot_encoding (10 points)\n",
    "1. Every word is represented as a vector containing 1 at its position in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(data_point_index, vocab_size):\n",
    "    # TO DO\n",
    "\n",
    "    # only the word index position is 1\n",
    "    temp = np.zeros(vocab_size, dtype = int)\n",
    "    temp[data_point_index] = 1\n",
    "\n",
    "\n",
    "    # END TO DO\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build_word_index_mapping (10 points)\n",
    "1. Given a word, the function should return the index of this word in dictionary.\n",
    "2. Given an index, the function should retrieve the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_index_mapping(corpus_dict):   \n",
    "    # TO DO\n",
    "\n",
    "    ind_2_word = []\n",
    "    value_list = []\n",
    "    index = 0\n",
    "    \n",
    "    # save words in ind_2_word, value_list is the list of indexes\n",
    "    for word in corpus_dict:\n",
    "        ind_2_word.append(word)\n",
    "        value_list.append(index)\n",
    "        index = index + 1\n",
    "    \n",
    "    # key is the word, value is the index\n",
    "    word_2_ind = dict((key, value) for (key, value) in zip(ind_2_word, value_list))\n",
    "\n",
    "\n",
    "    # END TO DO\n",
    "    return word_2_ind, ind_2_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the', 'royal', 'she', 'is', 'he', 'king', 'queen'}\n"
     ]
    }
   ],
   "source": [
    "word_2_ind, ind_2_word = build_word_index_mapping(corpus_dict)\n",
    "print(corpus_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Output:\n",
    "    \n",
    "1831\n",
    "1831\n",
    "1504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print (len(word_2_ind))\n",
    "print (len(ind_2_word))\n",
    "print (word_2_ind['he'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = corpus_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he is the king ', '  he is the royal king ', '  the king is royal ']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build_skip_pair (10 points)\n",
    "1. Build the word pair with given window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_skip_pair(window_size, sentences):\n",
    "    # TO DO\n",
    "\n",
    "    data = []\n",
    "    \n",
    "    for line in sentences:\n",
    "        \n",
    "        words = []\n",
    "        temp = line.split(\" \")\n",
    "        \n",
    "        # words in one sentence\n",
    "        for word in temp:\n",
    "            if word != \"\":\n",
    "                words.append(word)\n",
    "                \n",
    "        # build pairs for one sentence\n",
    "        length = len(words)\n",
    "        for i in range(length):\n",
    "            pair = []\n",
    "            for j in range(window_size):\n",
    "                if (i-(window_size-j)) >= 0:\n",
    "                    pair.append(words[i])\n",
    "                    pair.append(words[i-(window_size-j)])\n",
    "                    data.append(pair)\n",
    "                    pair = []\n",
    "            \n",
    "            for j in range(window_size):\n",
    "                if (i+(j+1)) < length:\n",
    "                    pair.append(words[i])\n",
    "                    pair.append(words[i+(j+1)])\n",
    "                    data.append(pair)\n",
    "                    pair = []\n",
    "\n",
    "\n",
    "    # END TO DO\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Input:\n",
    "\n",
    "He is the king .\n",
    "\n",
    "Example Output: \n",
    "\n",
    "[['he', 'is'], ['he', 'the'], ['is', 'he'], ['is', 'the'], ['is', 'king']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "[['he', 'is'], ['he', 'the'], ['is', 'he'], ['is', 'the'], ['is', 'king']]\n"
     ]
    }
   ],
   "source": [
    "data = build_skip_pair(WINDOW_SIZE, sentences)\n",
    "print (len(data))\n",
    "print (data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build_train_data_label\n",
    "1. Iterate all the word pairs in data\n",
    "2. Construct the train and label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_data_label(data, word_2_ind, vocab_size):\n",
    "    x_train = [] \n",
    "    y_train = [] \n",
    "    for data_word in data:\n",
    "        x_train.append(one_hot_encoding(word_2_ind[ data_word[0] ], vocab_size))\n",
    "        y_train.append(one_hot_encoding(word_2_ind[ data_word[1] ], vocab_size))\n",
    "    x_train = np.asarray(x_train)\n",
    "    y_train = np.asarray(y_train)\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = build_train_data_label(data, word_2_ind, len(corpus_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the dataset with batch size 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 7) (68, 7)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "\n",
    "dataset = dataset.shuffle(100).batch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyEmbeddingModel (20 points)\n",
    "1. init: Define all the layers you will use in the embedding network.\n",
    "2. call: Define the network layer connectivity:\n",
    "           - Fully connected with embedding_size/2 hidden neurons\n",
    "           - Batchnormalization (optional)\n",
    "           - Relu activation (optional)\n",
    "           - Fully connected with embedding_size hidden neurons (This should be the word embedding output)\n",
    "           - Batchnormalization (optional)\n",
    "           - Relu activation (optional)\n",
    "           - Fully connected that map to vocab_size output classes\n",
    "           - Softmax (This should be the classification output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEmbeddingModel(Model):\n",
    "  def __init__(self, embedding_size, vocab_size):\n",
    "    super(MyEmbeddingModel, self).__init__()\n",
    "    #Example:\n",
    "    \n",
    "    # TO DO\n",
    "    \n",
    "    self.vocab_size = vocab_size\n",
    "    self.embedding_size = embedding_size\n",
    "    self.d2 = Dense(embedding_size)\n",
    "    self.d3 = Dense(vocab_size, activation = 'softmax')\n",
    "    \n",
    "    # build embeddings\n",
    "    self.embeddings = tf.Variable(tf.random.uniform([vocab_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "    # END TO DO\n",
    "\n",
    "  def call(self, x):\n",
    "    #Example:\n",
    "        \n",
    "    # TO DO\n",
    "    \n",
    "    x_2 = self.d2(x)\n",
    "    x_3 = self.d3(x_2)\n",
    "\n",
    "    # END TO DO\n",
    "    return x_2, x_3 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "vocab_size = len(corpus_dict)\n",
    "model = MyEmbeddingModel(embedding_size, vocab_size)\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer (10 points)\n",
    "- Implement the SGD optimizer\n",
    "- Implement the RMSprop optimizer\n",
    "- Implement the Adagrad optimizer\n",
    "- Implement the Adadelta optimizer\n",
    "- Implement the Adam optimizer (Use the Adam optimizer for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: \n",
    "# optimizer = tf.keras.optimizers.Adamax()\n",
    "\n",
    "# optimizer = #SGD optimizer\n",
    "# optimizer = tf.keras.optimizers.SGD()\n",
    "\n",
    "# optimizer = #RMSprop optimizer\n",
    "# optimizer = tf.keras.optimizers.RMSprop()\n",
    "\n",
    "# optimizer = #Adagrad optimizer\n",
    "# optimizer = tf.keras.optimizers.Adagrad()\n",
    "\n",
    "# optimizer = #Adadelta optimizer\n",
    "# optimizer = tf.keras.optimizers.Adadelta()\n",
    "\n",
    "# optimizer = #Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training step. Calculate the loss and optimize the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        _, predictions = model(inputs, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss = tf.reduce_mean(loss)\n",
    "\n",
    "    return train_loss, labels, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Output:\n",
    "\n",
    "    Epoch 0, Loss: 6.393890857696533\n",
    "    Epoch 1, Loss: 5.391612529754639\n",
    "    Epoch 2, Loss: 4.996224403381348\n",
    "    Epoch 3, Loss: 4.692948341369629\n",
    "    Epoch 4, Loss: 4.473527908325195\n",
    "    Epoch 5, Loss: 4.335629940032959\n",
    "    Epoch 6, Loss: 4.251341342926025\n",
    "    Epoch 7, Loss: 4.205460071563721\n",
    "    Epoch 8, Loss: 4.172143936157227\n",
    "    Epoch 9, Loss: 4.1499714851379395\n",
    "    Epoch 10, Loss: 4.129685878753662\n",
    "    ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss.\n",
      "Epoch 0, Loss: 1.9169862270355225\n",
      "Epoch 1, Loss: 1.8538099527359009\n",
      "Epoch 2, Loss: 1.783963918685913\n",
      "Epoch 3, Loss: 1.7223995923995972\n",
      "Epoch 4, Loss: 1.6854405403137207\n",
      "Epoch 5, Loss: 1.6859817504882812\n",
      "Epoch 6, Loss: 1.6435712575912476\n",
      "Epoch 7, Loss: 1.5880597829818726\n",
      "Epoch 8, Loss: 1.5867924690246582\n",
      "Epoch 9, Loss: 1.5509724617004395\n",
      "Epoch 10, Loss: 1.5349841117858887\n",
      "Epoch 11, Loss: 1.5314093828201294\n",
      "Epoch 12, Loss: 1.5248355865478516\n",
      "Epoch 13, Loss: 1.537987232208252\n",
      "Epoch 14, Loss: 1.486838459968567\n",
      "Epoch 15, Loss: 1.4981067180633545\n",
      "Epoch 16, Loss: 1.4918311834335327\n",
      "Epoch 17, Loss: 1.459812879562378\n",
      "Epoch 18, Loss: 1.4931857585906982\n",
      "Epoch 19, Loss: 1.4732184410095215\n",
      "Epoch 20, Loss: 1.4576481580734253\n",
      "Epoch 21, Loss: 1.4823317527770996\n",
      "Epoch 22, Loss: 1.486828088760376\n",
      "Epoch 23, Loss: 1.4974586963653564\n",
      "Epoch 24, Loss: 1.4479212760925293\n",
      "Epoch 25, Loss: 1.4379048347473145\n",
      "Epoch 26, Loss: 1.4544243812561035\n",
      "Epoch 27, Loss: 1.4782058000564575\n",
      "Epoch 28, Loss: 1.4226057529449463\n",
      "Epoch 29, Loss: 1.4527901411056519\n",
      "Epoch 30, Loss: 1.4403393268585205\n",
      "Epoch 31, Loss: 1.4551076889038086\n",
      "Epoch 32, Loss: 1.4469685554504395\n",
      "Epoch 33, Loss: 1.4384806156158447\n",
      "Epoch 34, Loss: 1.4454500675201416\n",
      "Epoch 35, Loss: 1.4491859674453735\n",
      "Epoch 36, Loss: 1.4575225114822388\n",
      "Epoch 37, Loss: 1.444801688194275\n",
      "Epoch 38, Loss: 1.4081306457519531\n",
      "Epoch 39, Loss: 1.423133134841919\n",
      "Epoch 40, Loss: 1.453940510749817\n",
      "Epoch 41, Loss: 1.4434648752212524\n",
      "Epoch 42, Loss: 1.4587408304214478\n",
      "Epoch 43, Loss: 1.4081504344940186\n",
      "Epoch 44, Loss: 1.4088950157165527\n",
      "Epoch 45, Loss: 1.4271386861801147\n",
      "Epoch 46, Loss: 1.430103063583374\n",
      "Epoch 47, Loss: 1.4129513502120972\n",
      "Epoch 48, Loss: 1.4349664449691772\n",
      "Epoch 49, Loss: 1.4121484756469727\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    batch_loss = 0.0\n",
    "    num_batch = 0\n",
    "    for (batch, (inputs, labels)) in enumerate(dataset):\n",
    "        train_loss, labels, predictions = train_step(inputs, labels)\n",
    "        batch_loss += train_loss\n",
    "        num_batch += 1\n",
    "    template = 'Epoch {}, Loss: {}'\n",
    "    print(template.format(epoch, batch_loss/num_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build_embedding_dict (10 points)\n",
    "1. Iterate the corpus_dict and generate the embedding for each word.\n",
    "2. Use the trained model to generate the word embedding with given one-hot embedding word.\n",
    "3. Store the word and embedding in a dictionary. The key should be the word. The value should be the embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_dict(model, corpus_dict):\n",
    "    embeddings = dict()\n",
    "    # TO DO\n",
    "    \n",
    "    for word in corpus_dict:\n",
    "        word_id = word_2_ind[word]\n",
    "        # get embeddings for the current word\n",
    "        embed = model.embeddings[word_id]\n",
    "        # save it in the dictionary\n",
    "        embeddings[word] = embed\n",
    "\n",
    "    # example (use the trained model to generate the word embedding with given one-hot embedding word): sample_embedding, _ = model(sample_one_hot)\n",
    "\n",
    "    # END TO DO\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# euclidean_dist_np (10 points)\n",
    "1. Calculate the Euclidean distance between two input vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist_np(vec1, vec2):\n",
    "    dist = 0.0\n",
    "    # TO DO\n",
    "\n",
    "    # calculate the euclidean distance\n",
    "    dist = np.linalg.norm(vec1-vec2)\n",
    "\n",
    "    # END TO DO\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find_closest (10 points)\n",
    "1. Calculate the euclidean distance between the given word and all the words in embedding dictionary.\n",
    "2. Sort the dictionary by value in ascending order.\n",
    "3. Return the first three closet words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(word, embeddings):\n",
    "    result = dict()\n",
    "    # TO DO\n",
    "    \n",
    "    # save the euclidean distance between the current word's embeds and the target word's embeds in the dictionary\n",
    "    for key in embeddings:\n",
    "        result[key] = euclidean_dist_np(embeddings[key], embeddings[word])\n",
    "    \n",
    "    # sort the dictionary by value\n",
    "    result_list = sorted(result.items(), key=lambda x: x[1]) \n",
    "    \n",
    "    # END TO DO\n",
    "    return result_list[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Output: \n",
    "\n",
    "[('she', 0.0), ('he', 5.3993783), ('they', 5.7223315)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('she', 0.0), ('king', 14.313944), ('he', 14.318885)]\n"
     ]
    }
   ],
   "source": [
    "embedding_dict = build_embedding_dict(model, corpus_dict)\n",
    "print(find_closest('she', embedding_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize_cluster\n",
    "1. Visualize the word embedding in 2D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cluster(embedding_dict): \n",
    "    labels = []\n",
    "    tokens = []\n",
    "    for w in embedding_dict.keys():\n",
    "        labels.append(w)\n",
    "        tokens.append(embedding_dict[w])\n",
    "    tsne_model = TSNE(perplexity=10, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "\n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                         xy=(x[i], y[i]),\n",
    "                         xytext=(5, 2),\n",
    "                         textcoords='offset points',\n",
    "                         ha='right',\n",
    "                         va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cluster(embedding_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
